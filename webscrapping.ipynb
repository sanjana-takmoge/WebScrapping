{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab636edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup library is used for parsing HTML and XML documents.\n",
    "# It helps to extract data from web pages in an easy and structured way.\n",
    "from bs4 import BeautifulSoup  \n",
    "\n",
    "# requests library is used to send HTTP requests (GET, POST, etc.)\n",
    "# It helps to fetch the content of a website (HTML source code).\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9284e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending an HTTP GET request to the given URL\n",
    "# This fetches the HTML content of the fake jobs webpage for scraping\n",
    "page = requests.get('https://realpython.github.io/fake-jobs/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e88d52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a BeautifulSoup object to parse the HTML content of the webpage\n",
    "# page.content contains the raw HTML data\n",
    "# 'html.parser' is used to convert HTML into a searchable and structured format\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b3239f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding all HTML elements that have the class name 'card-content'\n",
    "# Each 'card-content' block represents a single job card on the webpage\n",
    "all_cards = soup.find_all(class_='card-content')\n",
    "\n",
    "# Counting the total number of job cards found on the webpage\n",
    "len(all_cards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d5ff880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"card-content\">\n",
       "<div class=\"media\">\n",
       "<div class=\"media-left\">\n",
       "<figure class=\"image is-48x48\">\n",
       "<img alt=\"Real Python Logo\" src=\"https://files.realpython.com/media/real-python-logo-thumbnail.7f0db70c2ed2.jpg?__no_cf_polish=1\"/>\n",
       "</figure>\n",
       "</div>\n",
       "<div class=\"media-content\">\n",
       "<h2 class=\"title is-5\">Senior Python Developer</h2>\n",
       "<h3 class=\"subtitle is-6 company\">Payne, Roberts and Davis</h3>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"content\">\n",
       "<p class=\"location\">\n",
       "        Stewartbury, AA\n",
       "      </p>\n",
       "<p class=\"is-small has-text-grey\">\n",
       "<time datetime=\"2021-04-08\">2021-04-08</time>\n",
       "</p>\n",
       "</div>\n",
       "<footer class=\"card-footer\">\n",
       "<a class=\"card-footer-item\" href=\"https://www.realpython.com\" target=\"_blank\">Learn</a>\n",
       "<a class=\"card-footer-item\" href=\"https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\" target=\"_blank\">Apply</a>\n",
       "</footer>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting the first job card from the list of all job cards\n",
    "# all_cards[0] represents the first job posting on the webpage\n",
    "card = all_cards[0]\n",
    "\n",
    "# Displaying the selected job card (HTML content)\n",
    "card\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deca8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the job designation/title from the selected job card\n",
    "# 'title is-5' class contains the job role name\n",
    "# .text is used to get only the readable text (without HTML tags)\n",
    "designation = card.find(class_='title is-5').text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15478e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Payne, Roberts and Davis'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the company name from the selected job card\n",
    "# 'subtitle is-6 company' class contains the company name\n",
    "# .text is used to extract readable text from the HTML element\n",
    "company_name = card.find(class_='subtitle is-6 company').text\n",
    "\n",
    "# Displaying the extracted company name\n",
    "company_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b2f89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the job location from the selected job card\n",
    "# 'location' class contains the job location details\n",
    "# .text extracts readable text from HTML\n",
    "# .strip('\\n') removes unwanted newline characters from the text\n",
    "location = card.find(class_='location').text.strip('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a786a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the job posting date from the selected job card\n",
    "# 'time' HTML tag contains the date when the job was posted\n",
    "# .text is used to get the readable date text\n",
    "date_of_post = card.find('time').text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6828112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the job apply link from the selected job card\n",
    "# 'card-footer-item' class contains multiple links (e.g., Learn more, Apply)\n",
    "# [1] is used to select the second link, which is the Apply link\n",
    "# ['href'] extracts the URL from the anchor tag\n",
    "apply_link = card.find_all(class_='card-footer-item')[1]['href']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67506423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty list to store all job records\n",
    "records = []\n",
    "\n",
    "# Looping through each job card extracted from the webpage\n",
    "for card in all_cards:\n",
    "    \n",
    "    # Extracting job designation/title\n",
    "    designation = card.find(class_='title is-5').text\n",
    "    \n",
    "    # Extracting company name\n",
    "    company_name = card.find(class_='subtitle is-6 company').text\n",
    "    \n",
    "    # Extracting job location and removing extra newline characters\n",
    "    location = card.find(class_='location').text.strip('\\n')\n",
    "    \n",
    "    # Extracting job posting date\n",
    "    date_of_post = card.find('time').text\n",
    "    \n",
    "    # Extracting the job apply link (second footer link)\n",
    "    apply_link = card.find_all(class_='card-footer-item')[1]['href']\n",
    "    \n",
    "    # Storing extracted job details in a dictionary\n",
    "    d1 = {\n",
    "        'designation': designation,\n",
    "        'company_name': company_name,\n",
    "        'location': location,\n",
    "        'date_of_post': date_of_post,\n",
    "        'apply_link': apply_link\n",
    "    }\n",
    "    \n",
    "    # Appending each job record to the records list\n",
    "    records.append(d1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8a7d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the current job record dictionary to the records list\n",
    "records.append(d1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b95f4523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas library for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Converting the list of job records (records) into a Pandas DataFrame\n",
    "# Each dictionary in the list becomes a row in the DataFrame\n",
    "df = pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94f2efbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the shape of the DataFrame\n",
    "# It returns the number of rows (jobs) and columns (features)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46bfa87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>company_name</th>\n",
       "      <th>location</th>\n",
       "      <th>date_of_post</th>\n",
       "      <th>apply_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Python Developer</td>\n",
       "      <td>Payne, Roberts and Davis</td>\n",
       "      <td>Stewartbury, AA\\n</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Energy engineer</td>\n",
       "      <td>Vasquez-Davidson</td>\n",
       "      <td>Christopherville, AA\\n</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legal executive</td>\n",
       "      <td>Jackson, Chambers and Levy</td>\n",
       "      <td>Port Ericaburgh, AA\\n</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fitness centre manager</td>\n",
       "      <td>Savage-Bradley</td>\n",
       "      <td>East Seanview, AP\\n</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product manager</td>\n",
       "      <td>Ramirez Inc</td>\n",
       "      <td>North Jamieview, AP\\n</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               designation                company_name  \\\n",
       "0  Senior Python Developer    Payne, Roberts and Davis   \n",
       "1          Energy engineer            Vasquez-Davidson   \n",
       "2          Legal executive  Jackson, Chambers and Levy   \n",
       "3   Fitness centre manager              Savage-Bradley   \n",
       "4          Product manager                 Ramirez Inc   \n",
       "\n",
       "                               location date_of_post  \\\n",
       "0               Stewartbury, AA\\n         2021-04-08   \n",
       "1          Christopherville, AA\\n         2021-04-08   \n",
       "2           Port Ericaburgh, AA\\n         2021-04-08   \n",
       "3             East Seanview, AP\\n         2021-04-08   \n",
       "4           North Jamieview, AP\\n         2021-04-08   \n",
       "\n",
       "                                          apply_link  \n",
       "0  https://realpython.github.io/fake-jobs/jobs/se...  \n",
       "1  https://realpython.github.io/fake-jobs/jobs/en...  \n",
       "2  https://realpython.github.io/fake-jobs/jobs/le...  \n",
       "3  https://realpython.github.io/fake-jobs/jobs/fi...  \n",
       "4  https://realpython.github.io/fake-jobs/jobs/pr...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the first 5 rows of the DataFrame\n",
    "# This helps to quickly preview the scraped job data\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e9643e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Python Programmer (Entry-Level)        3\n",
       "Ship broker                            2\n",
       "Legal executive                        2\n",
       "Manufacturing systems engineer         2\n",
       "Materials engineer                     2\n",
       "                                      ..\n",
       "Broadcast engineer                     1\n",
       "Neurosurgeon                           1\n",
       "Immigration officer                    1\n",
       "Structural engineer                    1\n",
       "Engineer, broadcasting (operations)    1\n",
       "Name: designation, Length: 92, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting how many times each job designation appears in the DataFrame\n",
    "# value_counts() gives frequency of unique values in the 'designation' column\n",
    "df['designation'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38f42da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import requests  # To send HTTP requests and fetch webpage content\n",
    "from bs4 import BeautifulSoup  # For parsing HTML content\n",
    "import pandas as pd  # For structuring data and saving to CSV/JSON\n",
    "import logging  # For logging info and errors during scraping\n",
    "\n",
    "# Setting up logging configuration\n",
    "# Logs will be saved in 'scraper.log' with timestamp, level, and message\n",
    "logging.basicConfig(\n",
    "    filename=\"scraper.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Target URL of the fake jobs webpage\n",
    "URL = \"https://realpython.github.io/fake-jobs/\"\n",
    "\n",
    "# Function to fetch HTML content of a given URL\n",
    "def fetch_page(url):\n",
    "    # Sending GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Raise exception if request fails (status code not 200)\n",
    "    response.raise_for_status()\n",
    "    # Return the raw HTML content\n",
    "    return response.text\n",
    "\n",
    "# Function to parse job listings from HTML\n",
    "def parse_jobs(html):\n",
    "    # Create BeautifulSoup object to parse HTML\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Find all job cards on the page\n",
    "    cards = soup.find_all(\"div\", class_=\"card-content\")\n",
    "    # Initialize list to store job data\n",
    "    records = []\n",
    "\n",
    "    # Loop through each job card to extract details\n",
    "    for card in cards:\n",
    "        # Extract job title\n",
    "        title = card.find(\"h2\", class_=\"title\").text.strip()\n",
    "        # Extract company name\n",
    "        company = card.find(\"h3\", class_=\"company\").text.strip()\n",
    "        # Extract job location\n",
    "        location = card.find(\"p\", class_=\"location\").text.strip()\n",
    "\n",
    "        # Store the extracted data in a dictionary\n",
    "        records.append({\n",
    "            \"title\": title,\n",
    "            \"company\": company,\n",
    "            \"location\": location\n",
    "        })\n",
    "    # Return the list of job records\n",
    "    return records\n",
    "\n",
    "# Function to save scraped data into CSV and JSON files\n",
    "def save_data(records):\n",
    "    # Convert list of dictionaries into a Pandas DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    # Save DataFrame to CSV file (without index column)\n",
    "    df.to_csv(\"jobs_data.csv\", index=False)\n",
    "    # Save DataFrame to JSON file with readable formatting\n",
    "    df.to_json(\"jobs_data.json\", orient=\"records\", indent=4)\n",
    "\n",
    "# Main function to run the scraper\n",
    "def main():\n",
    "    try:\n",
    "        # Fetch HTML content from the URL\n",
    "        html = fetch_page(URL)\n",
    "        # Parse jobs from the HTML content\n",
    "        records = parse_jobs(html)\n",
    "        # Save the parsed job data to CSV and JSON\n",
    "        save_data(records)\n",
    "        # Log successful scraping\n",
    "        logging.info(\"Scraping completed successfully\")\n",
    "    except Exception as e:\n",
    "        # Log any errors that occur during scraping\n",
    "        logging.error(f\"Error occurred: {e}\")\n",
    "\n",
    "# Entry point: run main function when script is executed\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "153500a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Saving the DataFrame to a CSV file named 'jobs_data.csv'\n",
    "# index=False ensures that the DataFrame index is not written to the file\n",
    "df.to_csv(\"jobs_data.csv\", index=False)\n",
    "\n",
    "# Saving the same DataFrame to a JSON file named 'jobs_data.json'\n",
    "# orient=\"records\" creates a list of dictionaries (one per row)\n",
    "# indent=4 makes the JSON file readable with proper indentation\n",
    "df.to_json(\"jobs_data.json\", orient=\"records\", indent=4)\n",
    "\n",
    "# Print confirmation message to indicate files have been saved successfully\n",
    "print(\"Files saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1b8f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import requests  # To send HTTP requests and fetch webpage content\n",
    "from bs4 import BeautifulSoup  # For parsing HTML content\n",
    "import pandas as pd  # For structuring data and saving to CSV/JSON\n",
    "\n",
    "# Function to fetch the HTML content of a given URL\n",
    "def fetch_page(url):\n",
    "    # Sending a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Raise an exception if the request failed (status code not 200)\n",
    "    response.raise_for_status()\n",
    "    # Return the HTML content of the page\n",
    "    return response.text\n",
    "\n",
    "# Function to parse job postings from the fetched HTML\n",
    "def parse_jobs(html):\n",
    "    # Create BeautifulSoup object to parse HTML content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Find all job cards on the page using the class 'card-content'\n",
    "    cards = soup.find_all(\"div\", class_=\"card-content\")\n",
    "\n",
    "    # Initialize an empty list to store job records\n",
    "    records = []\n",
    "    # Loop through each job card to extract job details\n",
    "    for card in cards:\n",
    "        # Extract job title and remove extra spaces\n",
    "        title = card.find(\"h2\", class_=\"title\").text.strip()\n",
    "        # Extract company name and remove extra spaces\n",
    "        company = card.find(\"h3\", class_=\"company\").text.strip()\n",
    "        # Extract job location and remove extra spaces\n",
    "        location = card.find(\"p\", class_=\"location\").text.strip()\n",
    "\n",
    "        # Store extracted details in a dictionary\n",
    "        records.append({\n",
    "            \"title\": title,\n",
    "            \"company\": company,\n",
    "            \"location\": location\n",
    "        })\n",
    "    # Return the list of all job records\n",
    "    return records\n",
    "\n",
    "# Function to save the extracted job data to CSV and JSON files\n",
    "def save_data(records):\n",
    "    # Convert the list of dictionaries into a Pandas DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    # Save the DataFrame to a CSV file (without index column)\n",
    "    df.to_csv(\"jobs_data.csv\", index=False)\n",
    "    # Save the same DataFrame to a JSON file with readable formatting\n",
    "    df.to_json(\"jobs_data.json\", orient=\"records\", indent=4)\n",
    "    # Return the DataFrame for further use if needed\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04bcd844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Python Developer</td>\n",
       "      <td>Payne, Roberts and Davis</td>\n",
       "      <td>Stewartbury, AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Energy engineer</td>\n",
       "      <td>Vasquez-Davidson</td>\n",
       "      <td>Christopherville, AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Legal executive</td>\n",
       "      <td>Jackson, Chambers and Levy</td>\n",
       "      <td>Port Ericaburgh, AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fitness centre manager</td>\n",
       "      <td>Savage-Bradley</td>\n",
       "      <td>East Seanview, AP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product manager</td>\n",
       "      <td>Ramirez Inc</td>\n",
       "      <td>North Jamieview, AP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title                     company              location\n",
       "0  Senior Python Developer    Payne, Roberts and Davis       Stewartbury, AA\n",
       "1          Energy engineer            Vasquez-Davidson  Christopherville, AA\n",
       "2          Legal executive  Jackson, Chambers and Levy   Port Ericaburgh, AA\n",
       "3   Fitness centre manager              Savage-Bradley     East Seanview, AP\n",
       "4          Product manager                 Ramirez Inc   North Jamieview, AP"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL of the webpage to scrape\n",
    "URL = \"https://realpython.github.io/fake-jobs/\"\n",
    "\n",
    "# Fetch the HTML content of the URL\n",
    "# Calls the fetch_page() function defined earlier\n",
    "html = fetch_page(URL)\n",
    "\n",
    "# Parse the fetched HTML to extract job postings\n",
    "# Calls the parse_jobs() function\n",
    "records = parse_jobs(html)\n",
    "\n",
    "# Save the extracted job data to CSV and JSON files\n",
    "# Also returns a Pandas DataFrame for immediate use\n",
    "df = save_data(records)\n",
    "\n",
    "# Display the first 5 rows of the DataFrame to verify the data\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a2c86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing logging library to track info and errors\n",
    "import logging\n",
    "\n",
    "# Setting up logging configuration\n",
    "# Logs will be written to 'scraper.log' with timestamp, level, and message\n",
    "logging.basicConfig(\n",
    "    filename=\"scraper.log\",\n",
    "    level=logging.INFO,  # INFO level will log info and errors\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Using try-except block to catch errors during scraping\n",
    "try:\n",
    "    # Fetch HTML content from the URL\n",
    "    html = fetch_page(URL)\n",
    "    # Parse job postings from the HTML\n",
    "    records = parse_jobs(html)\n",
    "    # Save extracted data to CSV and JSON files\n",
    "    save_data(records)\n",
    "    # Log a success message if scraping completes without error\n",
    "    logging.info(\"Scraping completed successfully\")\n",
    "except Exception as e:\n",
    "    # Log any error that occurs during scraping\n",
    "    logging.error(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b36495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import requests  # To fetch HTML content from web pages\n",
    "from bs4 import BeautifulSoup  # To parse HTML content\n",
    "import pandas as pd  # To structure data and save to CSV/JSON\n",
    "import logging  # To log info and errors during scraping\n",
    "\n",
    "# Setting up logging configuration\n",
    "# All logs will be written to 'scraper.log' with timestamp, log level, and message\n",
    "logging.basicConfig(\n",
    "    filename=\"scraper.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "URL = \"https://realpython.github.io/fake-jobs/\"\n",
    "\n",
    "# Function to fetch HTML content of a webpage\n",
    "def fetch_page(url):\n",
    "    # Send GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Raise an exception if the request failed\n",
    "    response.raise_for_status()\n",
    "    # Return the raw HTML content\n",
    "    return response.text\n",
    "\n",
    "# Function to parse job postings from HTML\n",
    "def parse_jobs(html):\n",
    "    # Create BeautifulSoup object for HTML parsing\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Find all job cards using their CSS class\n",
    "    cards = soup.find_all(\"div\", class_=\"card-content\")\n",
    "    # Initialize list to store job data\n",
    "    records = []\n",
    "\n",
    "    # Loop through each job card to extract details\n",
    "    for card in cards:\n",
    "        # Extract job title, company, and location\n",
    "        title = card.find(\"h2\", class_=\"title\").text.strip()\n",
    "        company = card.find(\"h3\", class_=\"company\").text.strip()\n",
    "        location = card.find(\"p\", class_=\"location\").text.strip()\n",
    "\n",
    "        # Store the extracted details in a dictionary\n",
    "        records.append({\n",
    "            \"title\": title,\n",
    "            \"company\": company,\n",
    "            \"location\": location\n",
    "        })\n",
    "\n",
    "    # Return the list of job records\n",
    "    return records\n",
    "\n",
    "# Function to save job data to CSV and JSON\n",
    "def save_data(records):\n",
    "    # Convert list of dictionaries into a Pandas DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    # Save DataFrame to CSV file\n",
    "    df.to_csv(\"jobs_data.csv\", index=False)\n",
    "    # Save DataFrame to JSON file with readable formatting\n",
    "    df.to_json(\"jobs_data.json\", orient=\"records\", indent=4)\n",
    "\n",
    "# Main function to execute the scraper\n",
    "def main():\n",
    "    try:\n",
    "        # Fetch HTML content\n",
    "        html = fetch_page(URL)\n",
    "        # Parse jobs from HTML\n",
    "        records = parse_jobs(html)\n",
    "        # Save the extracted job data\n",
    "        save_data(records)\n",
    "        # Log success message\n",
    "        logging.info(\"Scraping completed successfully\")\n",
    "    except Exception as e:\n",
    "        # Log any errors that occur during scraping\n",
    "        logging.error(f\"Error occurred: {e}\")\n",
    "\n",
    "# Entry point: run main() if this script is executed\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c156418c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requirements.txt created\n"
     ]
    }
   ],
   "source": [
    "# Define the required Python packages for this project\n",
    "requirements = \"\"\"requests\n",
    "beautifulsoup4\n",
    "pandas\n",
    "\"\"\"\n",
    "\n",
    "# Write the required packages to 'requirements.txt'\n",
    "# This allows anyone to install dependencies using:\n",
    "# pip install -r requirements.txt\n",
    "with open(\"requirements.txt\", \"w\") as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"requirements.txt created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4af3b7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md created\n"
     ]
    }
   ],
   "source": [
    "# Define the content of the README file\n",
    "readme = \"\"\"# Job Listings Web Scraper\n",
    "\n",
    "This project scrapes job listings from a public website using Python and BeautifulSoup.\n",
    "\n",
    "## Tools Used\n",
    "- Python\n",
    "- BeautifulSoup\n",
    "- Requests\n",
    "- Pandas\n",
    "\n",
    "## Features\n",
    "- Extracts job title, company, location\n",
    "- Saves data to CSV and JSON\n",
    "- Logging and error handling\n",
    "\n",
    "## How to Run\n",
    "pip install -r requirements.txt\n",
    "python scraper.py\n",
    "\n",
    "## Author\n",
    "Sanjana Takmoge\n",
    "\"\"\"\n",
    "\n",
    "# Write the README content to 'README.md' file\n",
    "with open(\"README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(readme)\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"README.md created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cc8bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
